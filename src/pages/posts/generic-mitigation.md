---
layout: '../../layout/article.astro'
title: 'ğŸš¨ En cas dâ€™urgence, brisez la glace : â€œgeneric mitigationsâ€ et gestion dâ€™incident'
pubDate: 2024-07-24
description: 'Cet article propose de dÃ©tailler quatre exemples de generic mitigation, qui mâ€™ont Ã©tÃ© bien utiles par le passÃ©.'
published: true
tags: [ "site reliability engineering", "production", "incident"]
---

> Cet article a Ã©tÃ© initialement publiÃ© sur le blog
> de [OCTO](https://blog.octo.com/en-cas-d%27urgence-brisez-la-glace--generic-mitigations-et-gestion-d%27incident).

Jâ€™ai une mauvaise nouvelle pour vous.

Demain, la semaine prochaine, le mois prochain ou celui dâ€™aprÃ¨s, dans lâ€™annÃ©e avec un peu de chance, votre systÃ¨me
informatique va rencontrer des incidents. Et la **[loi de Murphy](https://fr.wikipedia.org/wiki/Loi_de_Murphy)** ne nous
Ã©pargnant jamais, cet incident se produira toujours au pire moment possible.

Si le systÃ¨me en question est dâ€™importance secondaire, lâ€™impact sera faible, avec une urgence modÃ©rÃ©e, ce qui nous
laissera le temps de rÃ©agir et de trouver une solution pÃ©renne. Dans lâ€™idÃ©al, on aura un peu de monitoring Ã  notre
disposition pour dÃ©tecter puis agir avant que les utilisateurs ne sâ€™en rendent compte.

Mais si le systÃ¨me en question est crucial pour lâ€™entreprise, alors, **il vaut mieux Ãªtre prÃ©parÃ© Ã  gÃ©rer cette
situation
dâ€™incident**. La situation Ã©tant dÃ©jÃ  assez stressante, autant mettre toutes les chances de notre cÃ´tÃ©.

Sâ€™il est impossible dâ€™envisager toutes les sources, il est cependant possible de prÃ©voir quelques rÃ©ponses qui
permettront de protÃ©ger les utilisateurs en cas dâ€™incident.

Vous lâ€™aurez compris, dans cet article, nous allons parler de gestion dâ€™incident.
Plus prÃ©cisÃ©ment, nous allons parler de **_generic mitigations_**.

# Incident, rÃ©solution et mitigation

Puisque nous allons parler de gestion dâ€™incident, commenÃ§ons par du vocabulaire.

Il nâ€™existe pas de dÃ©finition universelle dâ€™un **_incident_**. En fonction du contexte, le mot incident peut couvrir des
rÃ©alitÃ©s et des urgences diffÃ©rentes. Cependant, pour le reste de lâ€™article, je vous propose la dÃ©finition suivante :

Un **incident** survient quand le systÃ¨me se retrouve dans un Ã©tat qui demande une intervention **immÃ©diate**, et que *
*les
utilisateurs de ce systÃ¨me sont impactÃ©s**.

La gestion dâ€™incident, câ€™est alors lâ€™ensemble des processus, personnes et actions mises en Å“uvre pour permettre
lâ€™identification de la (ou les) panne(s), pour assurer la communication, et essayer de limiter autant que possible
lâ€™impact sur les utilisateurs. Cette situation inhabituelle est souvent stressante pour les Ã©quipes et â€“ sans
prÃ©paration â€“ parfois redoutÃ©e.

Il y a beaucoup de choses Ã  dire sur le sujet, mais concentrons-nous aujourdâ€™hui sur deux concepts : **rÃ©solution** et
**mitigation**.

Si la **rÃ©solution** dâ€™un incident correspond au moment oÃ¹ les dysfonctionnements du systÃ¨me ainsi que ses effets de
bord
ont Ã©tÃ© gÃ©rÃ©s (au point oÃ¹ le systÃ¨me retrouve un Ã©tat stable), on parlera plutÃ´t de mitigation quand les solutions
dÃ©ployÃ©es visent Ã  contenir lâ€™effet (ou les effets) de lâ€™incident afin dâ€™impacter au minimum nos utilisateurs. Une
mitigation fait souvent office de solution transitoire.

Plus le systÃ¨me que lâ€™on opÃ¨re est complexe, plus il peut Ãªtre difficile dâ€™isoler la cause dâ€™un dysfonctionnement. Entre
un systÃ¨me monolithique servant des pages HTML, et une architecture distribuÃ©e Ã  base de chorÃ©graphie entre
microservices, les efforts dâ€™investigation demandÃ©s pourront passer de triviaux Ã  titanesques.

Et mÃªme si on aimerait bien maÃ®triser Ã  100% nos systÃ¨mes, il y aura toujours des facteurs imprÃ©visibles. Par exemple,
difficile dâ€™anticiper quâ€™un reportage au journal tÃ©lÃ©visÃ© qui parlerait dâ€™un de nos systÃ¨mes donne envie Ã  des milliers
de personnes de se connecter soudainement.

Il vaut mieux avoir quelques rÃ©ponses prÃ©vues dans sa besace, pour rÃ©agir **rapidement** et en **toute sÃ©curitÃ©**.

## Les â€œgeneric mitigationsâ€

Une _generic mitigation_, câ€™est une **mitigation prÃ©vue** qui vise Ã  rÃ©agir **rapidement** Ã  une large variÃ©tÃ© de pannes
qui pourraient impacter nos utilisateurs et dÃ©gÃ©nÃ©rer en incident.

La force de cette catÃ©gorie de mitigations, câ€™est quâ€™elles sont peu spÃ©cifiques, et permettent dâ€™agir sur une catÃ©gorie
de problÃ¨me et non un problÃ¨me prÃ©cis. Et câ€™est tout lâ€™intÃ©rÃªt ! **Pas besoin de comprendre lâ€™ensemble du problÃ¨me et
ses
implications pour activer une generic mitigation**.

Lâ€™ambition des _generic mitigations_ est de **se donner du temps** afin de rÃ©soudre le problÃ¨me plus sereinement. Il
faudra
malgrÃ© tout identifier la cause du problÃ¨me, trouver une solution et la mettre en place, mais elle permettra dâ€™Ã©pargner
nos utilisateurs durant lâ€™intervalle.

# Zoom sur 4 types de generic mitigation

Pour cet article, je vous propose de dÃ©tailler quatre exemples de generic mitigation, qui mâ€™ont Ã©tÃ© bien utiles par le
passÃ©. Ce ne sont que quatre exemples, il existe plein dâ€™autres faÃ§ons dâ€™imaginer des mitigations, gardez en tÃªte que la
liste nâ€™est pas exhaustive.

## Rollback

Cette mitigation est probablement la plus commune de toutes. Si automatiser le processus de dÃ©ploiement dâ€™une nouvelle
version est maintenant une pratique assez commune, le processus de retour Ã  la version prÃ©cÃ©dente nâ€™est pas toujours
aussi outillÃ©, et câ€™est bien dommage.

Dans le stress de lâ€™incident, il est tentant dâ€™essayer de trouver une solution rapidement pour pousser un correctif, en
approche move forward.

Le risque : proposer une solution basÃ©e sur une comprÃ©hension partielle et parfois dâ€™aggraver la situation.

**Recommandations :**

- Mettre en place un outillage pour Ãªtre capable de revenir rapidement Ã  la version prÃ©cÃ©dente
- Conserver les artefacts de la version (image ou tag) permettant de cibler une version particuliÃ¨re sans ambiguÃ¯tÃ©â€¦
  Croyez-moi, les commits id ne sont pas vos amis sous la pression.
- Essayer de rÃ©duire la taille des changements. Revenir en arriÃ¨re sur quelques jours de travail nâ€™est pas aussi
  impactant que quelques semaines, et permet une relative sÃ©rÃ©nitÃ©.

## SchÃ©ma rollback

Cette mitigation est une variante assez dÃ©licate de la prÃ©cÃ©dente. Lorsque le logiciel Ã©volue, la structure de nos bases
de donnÃ©es peut Ã©voluer en mÃªme temps (on parle de migration de base de donnÃ©es). Ces Ã©volutions impliquent souvent deux
Ã©tapes, la premiÃ¨re Ã©tant la mise Ã  jour du schema de la base de donnÃ©es et la seconde une adaptation des donnÃ©es
existantes.
Câ€™est une opÃ©ration sensible, car elle peut provoquer des indisponibilitÃ©s ou des ralentissements.

Le code et la structure de la base de donnÃ©es allant souvent de pair, il est parfois nÃ©cessaire de revenir Ã  la
structure prÃ©cÃ©dente. Malheureusement, cette partie nâ€™est pas si rÃ©guliÃ¨rement testÃ©e.

**Recommandation :**

- Tester la procÃ©dure de temps en temps sera salutaire le jour oÃ¹ il faudra le faire en production (Ce conseil est
  Ã©galement valable pour la mitigation prÃ©cÃ©dente).

**Pour aller plus loin :** Il existe des techniques permettant de limiter â€“ voire supprimer â€“ les ralentissements, voire
des
downtime liÃ©s aux migrations de schÃ©ma des donnÃ©es. On retrouve certaines de ces techniques sous le terme de [Zero
Downtime Deployment](https://blog.octo.com/zero-downtime-deployment).

## DÃ©grader la qualitÃ© de service

Une autre situation qui met parfois nos systÃ¨mes en Ã©chec, câ€™est quand ils doivent encaisser une charge plus importante
que prÃ©vu ou que lâ€™un des composants se met Ã  moins bien fonctionner que dâ€™ordinaire. Dans ces conditions, le service va
montrer des signes de faiblesses sous la forme de temps de rÃ©ponse qui se dÃ©gradent ou dâ€™erreurs qui augmentent sans
pour autant Ãªtre totalement hors service.

Par exemple, chez un de nos clients, notre application dÃ©pendait dâ€™un systÃ¨me partenaire de validation dâ€™identitÃ© pour
la crÃ©ation de compte. AprÃ¨s quelques secondes de validation, nous pouvions confirmer Ã  lâ€™utilisateur que la crÃ©ation
sâ€™Ã©tait correctement passÃ©e. Nous avions identifiÃ© que ce partenaire ne serait pas en mesure dâ€™encaisser une forte
sollicitation. Le site Ã©tant trÃ¨s mÃ©diatisÃ©, il Ã©tait probable que ce systÃ¨me ne tiendrait pas en cas dâ€™exposition
mÃ©diatique forte.

**Quelle option ?**
Lâ€™objectif de cette mitigation est de remplacer les opÃ©rations/features les plus coÃ»teuses (en CPU par exemple) par des
versions dÃ©gradÃ©es de celles-ci.

**Exemple : Temporiser les traitements**

Par exemple, on mettra en place des files permettant de temporiser le traitement. On pourra aussi sâ€™autoriser une
fraÃ®cheur de la donnÃ©e plus faible ou moins prÃ©cise. De cette maniÃ¨re, on soulage la tension sur le systÃ¨me tout en
continuant Ã  rendre le service, mais dans un mode dÃ©gradÃ©.

Notre solution avait Ã©tÃ© de protÃ©ger le systÃ¨me en question via un mÃ©canisme de seuil. En activant cette mitigation, les
utilisateurs Ã©taient informÃ©s que la validation de leur compte arriverait un peu plus tard. Lâ€™expÃ©rience est alors un
peu dÃ©gradÃ©e. Cependant, quand notre application est passÃ©e dans les mÃ©dias Ã  plusieurs reprises et que nous avons
activÃ© cette mitigation, le systÃ¨me est restÃ© opÃ©rationnel, y compris lors de lâ€™afflux massif dâ€™utilisateurs.

**Pour aller plus loin :**

Cette generic mitigation nÃ©cessite de prendre un peu de recul sur notre design et dâ€™identifier ce qui pourrait causer
des problÃ¨mes sous certaines conditions et voir sâ€™il est possible de rendre le service plus rÃ©silient. Il sâ€™agit
dâ€™adopter une philosophie de Design for failure et plus prÃ©cisÃ©ment ici, ce quâ€™on pourra qualifier de Graceful
degradation.

Pour dÃ©marrer, reprÃ©senter son architecture et identifier les risques est un bon point de dÃ©part. Des tests de
performance type stress test peuvent permettre de confirmer les hypothÃ¨ses ou dÃ©couvrir dâ€™autres points dâ€™attention.

En fonction des rÃ©sultats, il sâ€™agira dâ€™identifier si une generic mitigation est possible ou si dâ€™autres actions sont
nÃ©cessaires.

## Augmenter la taille du systÃ¨me (upsize)

Historiquement, augmenter la taille de lâ€™infrastructure Ã©tait une opÃ©ration longue et difficile, ce qui limitait notre
capacitÃ© Ã  faire croÃ®tre le systÃ¨me en cas de besoin. Il nâ€™Ã©tait donc pas inhabituel de provisionner plus de capacitÃ©s
en prÃ©vision, car notre capacitÃ© de rÃ©action Ã©tait rÃ©duite.

Cette Ã©poque a bien changÃ©. Avec la gÃ©nÃ©ralisation des techniques dâ€™infrastructure-as-code, des clouds providers et des
technologies de conteneurisation, il est maintenant possible de modifier (sous certaines conditions) le dimensionnement
de lâ€™infrastructure avec beaucoup plus de souplesse allant jusquâ€™Ã  lâ€™automatisation (ce quâ€™on appelle lâ€™auto-scaling).
On parlera de scaling horizontal quand il sâ€™agit dâ€™ajouter des rÃ©plicas ou des machines) ou vertical en augmentant le
CPU ou la RAM par exemple.

**Ce quâ€™il faut considÃ©rer**

Augmenter la taille dâ€™une machine ou modifier le nombre de rÃ©plicas nâ€™est pas une opÃ©ration triviale et implique dâ€™avoir
rendu son systÃ¨me scalable. Ce nâ€™est donc pas disponible â€œpar dÃ©fautâ€, et rendre un systÃ¨me scalable ne se fait pas
pendant la gestion dâ€™un incident (les prÃ©-requis Ã  la scalabilitÃ© sont nombreux, que je ne dÃ©taillerai pas ici).

De plus, sous la pression dâ€™un incident, il est tentant de croire que le scaling sera la rÃ©ponse ultime Ã  des problÃ¨mes
de surcharge et de performance. Seulement, votre systÃ¨me ne pourra pas sâ€™Ã©tendre uniformÃ©ment et certaines parties ne
pourront tout simplement pas scaler sereinement en pÃ©riode dâ€™incident.

Enfin, augmenter la taille dâ€™une infrastructure implique aussi des coÃ»ts Ã  surveiller.

**Un exemple : Quand le scaling nâ€™est plus une solution.**

Nous intervenions sur une application qui sâ€™occupait de calculs complexes Ã  partir dâ€™un outil de gestion de stock. Suite
Ã  une communication Instagram qui est devenue virale, le systÃ¨me a Ã©tÃ© progressivement de plus en plus sollicitÃ© et
lâ€™application a commencÃ© Ã  gÃ©nÃ©rer des erreurs. La puissance de calcul prÃ©vue nâ€™Ã©tant plus adaptÃ©e Ã  la demande, nous
lâ€™avons augmentÃ©e, ce qui a temporairement rÃ©glÃ© le problÃ¨me.

Manque de chance, le trafic a continuÃ© dâ€™augmenter jusquâ€™Ã  que ce soit au tour de lâ€™outil de gestion de stock de devenir
la cause du problÃ¨me. Malheureusement, impossible de faire croÃ®tre cet outil sans causer une indisponibilitÃ© totale de
la plateforme.

En scalant une partie du systÃ¨me, nous nâ€™avions fait que **dÃ©placer la tension** sur une autre section du systÃ¨me.

## Conclusion

On sous-estime souvent notre besoin de prÃ©paration quand il sâ€™agit de gÃ©rer un systÃ¨me en production. Puisquâ€™il nâ€™est
pas possible dâ€™anticiper quelle sera la prochaine panne, on est souvent mal armÃ© pour rÃ©agir quand celle-ci survient.

Câ€™est lÃ  quâ€™interviennent les *generic mitigation*! En mettant en place quelques outils, je dote mon systÃ¨me dâ€™un â€œred
buttonâ€ que je peux activer sereinement quand des dysfonctionnements menacent mes utilisateurs. Un sacrÃ© outil Ã  ne pas
sous-estimer !

Ã‰videmment, ce nâ€™est pas une solution Ã  tout. Les generic mitigations sont une faÃ§on de se prÃ©parer, mais il existe
dâ€™autres pratiques qui permettent dâ€™anticiper, dâ€™expÃ©rimenter, rÃ©duire notre temps de rÃ©action, etc. Notamment,
nâ€™oublions pas que pour debugger lâ€™incident et identifier la/les causes racines, investir sur lâ€™observabilitÃ© sera un
alliÃ© prÃ©cieux.

Pour avoir vÃ©cu des incidents avec et sans ces outils Ã  ma disposition, je ne peux que vous inviter Ã  envisager quels
seraient les â€œred buttonsâ€ dont vous auriez besoin, car ils se sont rÃ©vÃ©lÃ©s trÃ¨s prÃ©cieux pour nos utilisateursâ€¦ et
moi !

**Quelques sources annexes :**

- Lâ€™article [â€œGeneric Mitigationâ€](https://www.oreilly.com/content/generic-mitigations/) (EN) qui a inspirÃ© celui-ci
- Le livre â€œAnatomy of an incidentâ€ (Ayelet Sachto & Adrienne Walcer - OReilly)
- [Zero Downtime Deployment](https://blog.octo.com/zero-downtime-deployment)
- [SRE : Les bonnes pratiques pour amÃ©liorer la fiabilitÃ© de ses services](https://blog.octo.com/sre-les-bonnes-pratiques-pour-ameliorer-la-fiabilite-de-ses-services)
